ğŸ’¬ 1. Warm-up & Introduction
- On senior level interview question will be focused on scenario based rather than what, focus will be on why.
Q1: Give me a brief introduction about yourself.

What to look for: How clearly they tell their story, sense of ownership, tech journey, leadership signs.

âœ… Good Answer Example:

â€œIâ€™ve been in DevOps for about 8 years, starting from system administration, then into CI/CD and Kubernetes automation. In my current role, I lead a small platform engineering team responsible for developer productivity, GitOps adoption, and cluster security. I enjoy solving cross-team problems â€” things that improve how developers ship code faster and safer.â€

ğŸ§­ 2. Day-to-Day Work
Q2: What does a typical day in your current role look like?

What to look for: Balance between strategic and hands-on work; team leadership, not just tool usage.

âœ… Good Answer Example:

â€œMy mornings usually start with reviewing overnight builds, monitoring alerts, and any failed deployments. Midday is mostly syncs â€” planning sprint goals, coordinating with developers, and reviewing infra PRs. Afternoons I spend on design reviews, automation improvements, or mentoring team members. I also ensure our GitOps environments stay in sync with whatâ€™s deployed.â€

ğŸ¯ 3. Roles & Responsibilities
Q3: What are your key roles and responsibilities?

What to look for: Breadth across CI/CD, infra, observability, and culture.

âœ… Good Answer Example:

â€œI oversee CI/CD pipeline design, manage Kubernetes clusters, and handle infrastructure-as-code using Terraform. I also work closely with InfoSec for compliance, own our observability stack, and mentor DevOps engineers to align on best practices.â€

ğŸ§© 4. Goals and Strategic Vision
Q4: What are your goals for this year â€” both personal and team-oriented?

What to look for: Growth mindset, measurable goals, alignment with business outcomes.

âœ… Good Answer Example:

â€œOur key goal is to bring full GitOps maturity â€” moving from semi-manual promotion to complete automation via ArgoCD. Personally, Iâ€™m focusing on improving our developer onboarding experience and setting up self-service CI/CD templates to reduce delivery friction.â€

ğŸ” 5. Problem-Solving Mindset
Q5: Whatâ€™s the most interesting challenge you solved recently?

What to look for: Real ownership, clarity of thought, and cross-functional collaboration.

âœ… Good Answer Example:

â€œWe had an issue where our CI pipeline time ballooned from 25 to 60 minutes. I traced it to unoptimized Docker layer caching in our Tekton tasks. I refactored them to use Kaniko caching and centralized artifact storage â€” reducing build time to 14 minutes. It improved developer feedback loops massively.â€

ğŸ§  6. Anticipation & Foresight
Q6: Whatâ€™s the most interesting challenge you want to solve next?

What to look for: Strategic curiosity â€” beyond immediate tickets.

âœ… Good Answer Example:

â€œWeâ€™re moving toward multi-cluster deployment models. I want to standardize how we manage cross-cluster GitOps syncs and secure service-to-service communication using SPIFFE/SPIRE.â€

ğŸ† 7. Pride & Impact
Q7: Whatâ€™s one thing youâ€™re most proud of implementing?

What to look for: End-to-end ownership, long-term impact, not just a â€œtoolâ€ they used.

âœ… Good Answer Example:

â€œIâ€™m proud of introducing policy-as-code using OPA Gatekeeper. Before that, we had no guardrails. Now we enforce naming conventions, label compliance, and image provenance checks â€” all through Git. It reduced manual code reviews and improved our compliance posture.â€

ğŸ“ˆ 8. Metrics & Accountability
Q8: What KPIs or metrics define your success as a DevOps Lead?

What to look for: Business-aligned KPIs, not just technical vanity metrics.

âœ… Good Answer Example:

â€œWe track deployment frequency, mean time to recovery, and change failure rate â€” basically DORA metrics. In addition, I monitor pipeline duration and developer satisfaction surveys to measure how DevOps impacts productivity.â€


â€œHow do you balance firefighting vs long-term improvements?â€

â€œWhat would you automate if you had one more engineer on your team?â€

â€œIf you could redo one part of your platform setup, what would it be and why?â€

â€œHow do you align DevOps priorities with product goals?



ğŸš€ Section 2: Transition to Technical Deep Dives

Youâ€™re no longer confirming their rÃ©sumÃ© â€” youâ€™re testing judgment and decision-making.
Use â€œscenario-styleâ€ questions that start broad, then zoom into detail.

ğŸ§± 1. Kubernetes Architecture & Troubleshooting
Q1: If a deployment is healthy in Kubernetes, but users report the app is slow â€” where do you start debugging?

What youâ€™re testing: end-to-end thinking â€” networking, app profiling, observability.

âœ… Strong Answer:

â€œFirst, Iâ€™d confirm pod health (readiness/liveness), then check node-level resource utilization. Next, Iâ€™d inspect service-level latency in metrics (Prometheus/Datadog). If pods are fine, Iâ€™d trace network hops using kubectl exec curl or distributed tracing tools like Jaeger. Often, itâ€™s a backend DB or ingress bottleneck rather than K8s itself.â€

ğŸ’¡ Follow-up: â€œHow would you prove itâ€™s not a Kubernetes problem?â€

Q2: How would you design multi-cluster or multi-region Kubernetes deployment?

âœ… Strong Answer:

â€œIâ€™d use a GitOps-driven approach with ArgoCD ApplicationSets across clusters. Networking and service discovery could be handled by a service mesh (Istio/Linkerd). For DR, Iâ€™d replicate stateful data via cross-region RDS and manage DNS failover using Route53 or ExternalDNS. Cluster registration and policy enforcement can go via Red Hat ACM or Anthos.â€

ğŸ’¬ Follow-up: â€œHow do you ensure secret consistency across these clusters?â€

âš™ï¸ 2. CI/CD & Pipeline Design
Q3: How do you ensure that your pipelines are reliable, secure, and maintainable at scale?

âœ… Strong Answer:

â€œWe treat pipelines as code and version them in Git. All templates are reusable YAMLs in a shared catalog. We run security scans via roxctl and Trivy before deployment. We also sign our container images with Cosign and enforce signature verification using an admission controller. Every pipeline run emits metrics to Grafana for DORA tracking.â€

ğŸ’¬ Follow-up: â€œWhatâ€™s one improvement you made recently that reduced build time?â€

Q4: How do you handle rollbacks in a GitOps setup?

âœ… Strong Answer:

â€œIn GitOps, rollback = Git revert. We revert the last commit in the prod branch and ArgoCD syncs the cluster back to the known state. For safety, we tag every deployment commit and link it with an artifact SHA, so rollback is both versioned and auditable.â€

ğŸ’¬ Follow-up: â€œWhatâ€™s the biggest challenge you faced while adopting GitOps?â€

â˜ï¸ 3. Cloud, IaC & Cost Optimization
Q5: How do you ensure Infrastructure as Code remains clean and consistent across environments?

âœ… Strong Answer:

â€œWe enforce structure using Terraform modules, remote state in S3 with DynamoDB locks, and policy-as-code checks with OPA or Terraform Cloud Sentinel. Each environment â€” dev, stage, prod â€” is created via the same modules, only with different variables.â€

ğŸ’¬ Follow-up: â€œWhatâ€™s your approach to handle drift?â€

Q6: How do you balance cost optimization with reliability in AWS or GCP?

âœ… Strong Answer:

â€œWe right-size instances using CloudWatch data, use spot nodes for stateless workloads, and autoscaling groups for elasticity. For reliability, we never mix spot with stateful services. Also, we schedule non-critical dev clusters to shut down at night.â€

ğŸ’¬ Follow-up: â€œHow do you show cost impact of your changes to leadership?â€

ğŸ” 4. Security, Compliance & Governance
Q7: How do you manage secrets securely across CI/CD and Kubernetes?

âœ… Strong Answer:

â€œWe use External Secrets Operator to fetch from Vault or AWS Secrets Manager. Secrets never touch Git. In pipelines, we inject them via sealed secrets or vault agent sidecars. For audit, all access is logged and rotated automatically.â€

ğŸ’¬ Follow-up: â€œWhat happens if your Vault goes down during deployment?â€

Q8: How do you enforce cluster compliance and policies?

âœ… Strong Answer:

â€œWe use OPA Gatekeeper policies as code â€” things like mandatory labels, disallowing latest tags, and requiring signed images. All PRs trigger a policy validation pipeline before merging. For runtime, we integrate with ACS to catch drifted containers.â€

ğŸ’¬ Follow-up: â€œHow do you handle exceptions to these policies?â€

ğŸ§  5. Reliability, Monitoring & SRE Mindset
Q9: What SLOs or metrics do you use to measure system health?

âœ… Strong Answer:

â€œWe define SLOs on latency (p95), error rate, and availability. These are tracked via Prometheus alerts and visualized in Grafana. Error budgets drive release cadence â€” if we breach, we freeze feature releases and focus on reliability.â€

ğŸ’¬ Follow-up: â€œHow do you design alerts that are actionable, not noisy?â€

Q10: Tell me about a real incident you handled â€” how did you identify the root cause?

âœ… Strong Answer:

â€œWe had intermittent 5xxs in prod. I correlated ingress logs with application traces and found DB connection pool exhaustion after a new release. We fixed it by tuning the pool and adding circuit breakers. Later, we added automated load testing to catch it earlier.â€

ğŸ’¬ Follow-up: â€œWhat changed in your process after that incident?â€

ğŸ§© 6. Leadership & Culture
Q11: How do you help developers adopt DevOps best practices without forcing it?

âœ… Strong Answer:

â€œI build empathy â€” show how CI/CD changes reduce their pain. We create internal templates and dashboards so they can onboard faster. Also, we highlight success stories in sprint reviews â€” it helps drive organic adoption.â€

ğŸ’¬ Follow-up: â€œWhatâ€™s one DevOps practice you think teams overcomplicate?â€

Q12: How do you balance hands-on work vs team leadership?

âœ… Strong Answer:

â€œI reserve 40% for hands-on work (to stay grounded) and 60% for mentorship, architecture reviews, and strategy. I ensure my team has autonomy, but Iâ€™m always in the loop through code reviews and design discussions.â€

ğŸ’¬ Follow-up: â€œWhen do you step in vs let the team struggle?â€

ğŸ§­ 1. Strategic Vision & Ownership
Q1: How do you decide what to automate and what not to?

ğŸ¯ Tests: Prioritization, ROI mindset, long-term thinking.

âœ… Strong Answer:

â€œI focus on frequency and risk. If something is repetitive and error-prone, automation is worth it. But if itâ€™s done once per quarter, I evaluate ROI first. I also consider how the automation will be maintained â€” because unmanaged scripts quickly turn into tech debt.â€

ğŸ’¬ Follow-up:

â€œCan you give a recent example where you chose not to automate something and why?â€

Q2: How do you define the success of a DevOps initiative?

ğŸ¯ Tests: Ability to tie DevOps to business outcomes.

âœ… Strong Answer:

â€œSuccess isnâ€™t a faster pipeline â€” itâ€™s faster value delivery. I measure by DORA metrics (deployment frequency, MTTR, failure rate) and developer feedback. If engineers are shipping safely and confidently, the initiative succeeded.â€

ğŸ’¬ Follow-up:

â€œHow do you communicate DevOps success to non-technical leadership?â€

ğŸ‘¥ 2. Team Leadership & Mentorship
Q3: How do you mentor mid-level DevOps engineers?

ğŸ¯ Tests: Coaching vs micromanagement.

âœ… Strong Answer:

â€œI start by pairing with them on real tasks â€” not classroom sessions. I ask questions instead of giving answers: â€˜What do you think is causing this?â€™ That builds independent debugging habits. Later, I review their PRs with comments focused on why, not just what.â€

ğŸ’¬ Follow-up:

â€œWhatâ€™s one thing you learned from a junior engineer recently?â€

Q4: How do you handle disagreements with developers or architects?

ğŸ¯ Tests: Emotional intelligence, collaboration under tension.

âœ… Strong Answer:

â€œI focus on shared goals. For example, if a developer insists on manual deployments, I donâ€™t argue about â€˜DevOps best practices.â€™ I ask what risk theyâ€™re trying to avoid. Usually, we can automate that safety check and solve the root concern.â€

ğŸ’¬ Follow-up:

â€œTell me about a time you were wrong in such a disagreement. What did you change afterward?â€

âš–ï¸ 3. Decision-Making Under Pressure
Q5: Describe how you handled a major production incident.

ğŸ¯ Tests: Crisis composure, process discipline, learning mindset.

âœ… Strong Answer:

â€œOnce, an SSL certificate expired and brought down our public API. We restored service by rolling back to a previous cert version, then automated certificate renewal with cert-manager. Postmortem focused on process gaps â€” now we have alerts for all expiring certs.â€

ğŸ’¬ Follow-up:

â€œWhat did you not do during that incident that you wish you had?â€

Q6: How do you handle conflicts between reliability and delivery speed?

ğŸ¯ Tests: SRE thinking, business empathy.

âœ… Strong Answer:

â€œWe track error budgets. If weâ€™re within budget, we push features; if weâ€™re breaching, we pause releases and fix reliability. This way, decisions are data-driven, not emotional.â€

ğŸ’¬ Follow-up:

â€œHow do you convince product managers to slow down when necessary?â€

ğŸŒ 4. Cross-Team Collaboration & Influence
Q7: How do you influence DevOps adoption in teams that resist change?

ğŸ¯ Tests: Change management, communication.

âœ… Strong Answer:

â€œI start small â€” one team, one win. For example, we automated test environments for one project; once others saw the time savings, adoption followed organically. You canâ€™t mandate culture; you have to prove its value.â€

ğŸ’¬ Follow-up:

â€œHow do you handle teams that still donâ€™t buy in after success stories?â€

Q8: How do you handle requests from multiple teams when priorities clash?

ğŸ¯ Tests: Leadership triage and transparency.

âœ… Strong Answer:

â€œI use an intake board and quantify impact â€” how many developers benefit, whatâ€™s the risk, and whatâ€™s the business value. Then I publish a transparent roadmap. Teams may not always like the prioritization, but they appreciate fairness.â€

ğŸ’¬ Follow-up:

â€œHow do you communicate â€˜noâ€™ without burning bridges?â€

ğŸ§© 5. Vision for the Platform
Q9: Whatâ€™s your vision for a modern internal developer platform?

ğŸ¯ Tests: Strategic architecture mindset.

âœ… Strong Answer:

â€œA self-service platform where developers can deploy apps without touching infra â€” using templates, GitOps, and clear guardrails. The platform abstracts Kubernetes complexity while keeping policy compliance. My goal is to make shipping software boring.â€

ğŸ’¬ Follow-up:

â€œWhatâ€™s one feature youâ€™d never put in such a platform â€” and why?â€

Q10: If you join our company, what would you focus on in the first 90 days?

ğŸ¯ Tests: Leadership planning and impact thinking.

âœ… Strong Answer:

â€œFirst, Iâ€™d learn â€” current infra, people, and pain points. Then Iâ€™d identify one or two high-impact wins â€” like reducing CI time or improving visibility with better observability. Iâ€™d also document our delivery process end-to-end and propose a roadmap with measurable milestones.â€

ğŸ’¬ Follow-up:

â€œWhatâ€™s a red flag youâ€™d look for in an existing DevOps setup?â€



Bonus â€œCurveballâ€ Reflection Questions

Use 1â€“2 of these near the end to gauge authenticity and creative thinking:

â€œWhatâ€™s one DevOps buzzword you think is misunderstood or overused?â€

â€œIf you had to remove one tool from your stack, what would it be and why?â€

â€œWhatâ€™s a non-technical skill that made you a better engineer?â€

â€œHow do you keep your team motivated when everythingâ€™s on fire?â€

â€œWhat does DevOps culture mean to you â€” in one sentence?â€



Kubernetes & Cloud-Native Architecture

1. If your Kubernetes workloads randomly go into CrashLoopBackOff but kubectl logs and describe show nothing obvious â€” how do you debug it?
Real debugging depth, not surface-level commands.
Iâ€™d start by checking events and initContainers, then inspect container exit codes and recent image changes.
Next, Iâ€™d check if itâ€™s a startup or readiness issue (often caused by bad configs, DB unavailability, or missing Secrets).
Then, Iâ€™d run the same container locally with docker run or podman run to reproduce.
If still unclear, Iâ€™d increase logging, use an ephemeral debug pod (kubectl debug), or use a sidecar to attach to the same namespace network for visibility.
Finally, Iâ€™d inspect resource limits and OOM kills from the nodeâ€™s journalctl or dmesg.

Is it possible to run a single service across multiple Kubernetes clusters? How? 
Tests: Multi-cluster awareness.
Not natively, since each cluster is independent. But you can expose a multi-cluster service via service mesh (Istio, Linkerd) or DNS-based load balancing (external-dns + GSLB).
For stateful components, Iâ€™d use multi-region data replication instead of cluster merging.
Tools like Submariner or KubeFed also help federate workloads across clusters.

How would you design HA (high availability) for a Kubernetes control plane?
Tests: Architectural understanding.
Iâ€™d use 3 or 5 control plane nodes behind a load balancer, with etcd quorum of odd numbers for consistency.
Distribute nodes across zones.
Backup etcd regularly (snapshot + S3).
Use cloud-managed control planes (EKS, GKE, ROSA) where the HA is built-in.
For on-prem, automate recovery with kubeadm + etcd restore scripts.

âš™ï¸ CI/CD & Automation

How do you prevent â€œpipeline driftâ€ when your Tekton or Jenkins pipelines differ slightly across projects?
Tests: Understanding of pipeline standardization & GitOps.
Iâ€™d use Pipeline as Code with reusable templates stored in a central repo, versioned with Git tags.
Each project references a pinned tag or commit (to prevent accidental drift).
For Tekton, Iâ€™d use remote tasks or ClusterTasks, combined with Catalog management and PipelineRun bundles to version control tasks.
Governance enforced via PR reviews and automated linting (e.g., Tekton Linter or Conftest).

If your build pipeline is stable but the app feels slow in production, whatâ€™s your first 30-minute diagnostic checklist?
Tests: Incident management under pressure.

Check latency at ingress/load balancer level.

Inspect DB response times and query metrics.

Correlate slow requests in APM (Datadog, Tempo, New Relic).

Compare pod CPU throttling and GC pauses.

Verify that autoscaling (HPA/VPA) is kicking in.

Look for network issues, DNS delays, or external API latency.

If all else fails â€” tcpdump or sidecar debugging to isolate packet loss.


Explain the difference between service mesh and API gateway â€” when do you use one vs. both?
Tests: Cloud-native design judgment.

A service mesh (like Istio) operates at Layer 7 for service-to-service communication inside the mesh â€” handles observability, retries, and mTLS.
An API gateway (like Kong, Apigee) sits at the edge, handling external requests, auth, rate limits.
Use both: gateway for north-south traffic, mesh for east-west traffic.

You discover your pods are rescheduled frequently but CPU/memory look fine â€” what else can cause this?
Tests: Depth in cluster behavior.

Could be node pressure due to:

Ephemeral storage exhaustion (df -h /var/lib/kubelet).

PID limits reached.

Node taints or eviction thresholds (diskPressure, memoryPressure).

Pod disruption budgets conflicting with node upgrades.

Pod anti-affinity rules forcing relocations.

ğŸ” Security & DevSecOps

How would you implement image signing and verification in your CI/CD pipeline?

Tests: DevSecOps and supply chain awareness.

Iâ€™d integrate Sigstore (cosign) or Red Hatâ€™s cosign policy with ACS in the pipeline.
The image is signed post-build using the pipelineâ€™s service account key, and admission policies (OPA Gatekeeper or ACS enforcement) reject unsigned images at deploy time.
This enforces provenance and non-repudiation.

How do you detect vulnerabilities in running containers in Kubernetes?
Tests: Runtime security & compliance knowledge.
Static scanning via Trivy or Grype only helps pre-deploy. For runtime, Iâ€™d use Red Hat ACS, Falco, or Aqua Security â€” they continuously scan images, monitor system calls, and alert on CVEs discovered post-deployment.
Combine that with admission policies and network isolation (Calico policies) for layered defense.

ğŸ§  Leadership & Design Thinking
If your team has 5 different pipelines, all working but inconsistent â€” how do you bring standardization without slowing innovation?
Tests: Technical leadership and communication skills.

Iâ€™d start with a pipeline framework â€” reusable templates, shared libraries, and guidelines â€” not rigid mandates.
Gradually move toward a GitOps model so pipelines become declarative.
Encourage engineers to contribute improvements via PRs.
Track metrics (build time, success rate, recovery time) to show measurable improvement.
The goal is to build a DevOps platform mindset, not a â€œgatekeeperâ€ culture.




ğŸ§© 1. Kubernetes Deep Dive (10 Questions)

1ï¸âƒ£ What happens when you run kubectl apply behind the scenes?

The client computes a patch (3-way merge between live, last-applied, and new configs) and sends it to the API server. etcd stores the desired state, and controllers reconcile it.
Tests understanding of declarative model and reconciliation loop.

2ï¸âƒ£ Explain the Kubernetes control loop and reconciliation concept.

Controllers constantly compare desired vs. current state from etcd and perform corrective actions.
Tests: true understanding of Kubernetesâ€™ core philosophy â€” not â€œjust Pods.â€

3ï¸âƒ£ Why do we need a CNI plugin, and how does pod networking work?

CNI (Container Network Interface) sets up pod network namespaces, veth pairs, and routes.
Every pod gets its own IP; CNIs like Calico or Cilium manage cross-node routing.
Tests: networking internals.

4ï¸âƒ£ How would you troubleshoot a Pod stuck in ContainerCreating?

Check:

kubectl describe events for ImagePullBackOff or MountVolume errors

Node journalctl -u kubelet logs

CNI readiness

PVC provisioning status

Quota/LimitRange constraints

5ï¸âƒ£ Explain difference between Deployment, StatefulSet, and DaemonSet.

Deployment â€“ stateless replicas.
StatefulSet â€“ stable network IDs, ordered rollout, persistent volumes.
DaemonSet â€“ runs one pod per node (e.g., logging/monitoring agents).

6ï¸âƒ£ Can we join two Kubernetes clusters?

No â€” clusters are independent control planes.
Use KubeFed, Service Mesh, Red Hat ACM, or GitOps for federation.

7ï¸âƒ£ How do you manage secrets securely in Kubernetes?

Avoid plaintext in etcd; use Sealed Secrets, External Secrets Operator, or Vault CSI Driver.
Integrate with IAM/KMS for encryption at rest.

8ï¸âƒ£ How do you debug a network issue between pods on different nodes?

Check CNI status and routes

Use kubectl exec + ping or curl

Inspect iptables or cilium monitor

Validate node-to-node connectivity

9ï¸âƒ£ What are PDBs and PSP replacements in modern K8s?

Pod Disruption Budgets limit voluntary evictions; PSPs are deprecated â€” replaced by OPA Gatekeeper or Kyverno for policy control.

ğŸ”Ÿ Explain Horizontal vs. Vertical Pod Autoscaler vs. Cluster Autoscaler.

HPA: adjusts replicas based on metrics

VPA: adjusts pod CPU/memory requests

CA: scales node count dynamically

âš™ï¸ 2. CI/CD, GitOps & Automation (10 Questions)

1ï¸âƒ£ How would you enforce pipeline consistency across teams?

Store pipeline templates centrally, version via Git tags, and reference via commit SHA.
Use Tekton remote tasks or Jenkins shared libraries.

2ï¸âƒ£ Whatâ€™s the difference between Continuous Delivery and Continuous Deployment?

Delivery stops at approval â†’ human gate; Deployment auto-releases to prod.
Senior engineers understand why you might choose one over the other.

3ï¸âƒ£ How do you handle secret injection in pipelines?

Use sealed secrets, vault integrations, or service account-based secret mounts.
Never inject plaintext env vars.

4ï¸âƒ£ How do you ensure idempotent pipelines?

Declarative definitions, stateless stages, artifact versioning, and rollback safety via GitOps.

5ï¸âƒ£ How do you roll back a failed deployment in GitOps?

Revert the Git commit â†’ ArgoCD detects drift â†’ syncs back automatically.
Observability confirms rollback success.

6ï¸âƒ£ How to reduce build time for large Docker images?

Use multi-stage builds, .dockerignore, and layer caching.
In CI, enable remote caching (BuildKit, Kaniko cache, or Podman registry cache).

7ï¸âƒ£ How would you trigger a Tekton pipeline automatically when a Git PR is merged?

Use Pipeline-as-Code (PAC) feature or GitHub Webhook â†’ triggers Tekton PipelineRun.
Secure with event listener and service account.

8ï¸âƒ£ Whatâ€™s your approach to managing pipeline drift?

Central catalog, pinned versions, GitOps control of tasks, and pipeline governance.

9ï¸âƒ£ How do you test pipeline changes safely?

Use preview environments, feature branches, and sandbox clusters.
Validate pipelines with tkn pipeline dry-run or pre-commit linting.

ğŸ”Ÿ How would you integrate security scanning into CI/CD?

Add roxctl or Trivy scan stage post-build, before push; fail build on CVE threshold.
Push results to ACS, Jira, or DefectDojo.

â˜ï¸ 3. Cloud & Infrastructure as Code (10 Questions)

1ï¸âƒ£ How would you design multi-region disaster recovery?

Cross-region replication (RDS, S3), IaC-driven infra rebuild (Terraform), GitOps for apps, global DNS failover.

2ï¸âƒ£ Terraform vs. Ansible â€“ when to use which?

Terraform â†’ infra provisioning (declarative).
Ansible â†’ configuration management (imperative).
Use both: Terraform to create servers, Ansible to configure them.

3ï¸âƒ£ How do you handle Terraform state locking and version control?

Use remote backend (S3 + DynamoDB lock or Terraform Cloud) and branch-based workflows.

4ï¸âƒ£ Explain blue-green vs. canary deployment.

Blue-green â†’ full swap.
Canary â†’ gradual rollout.
Choose based on traffic volume and risk tolerance.

5ï¸âƒ£ How do you manage cross-account access securely in AWS?

IAM roles with trust policies, not static keys.
AssumeRole via STS for automation.

6ï¸âƒ£ How to optimize AWS cost for EKS workloads?

Use spot instances for non-critical workloads, autoscaling groups, pod requests tuning, and cluster rightsizing.

7ï¸âƒ£ How would you build an immutable infrastructure model?

Golden AMIs or image-based deployments via Packer and pipeline builds.
Replace, donâ€™t patch.

8ï¸âƒ£ Whatâ€™s your IaC testing strategy?

Use terraform plan CI checks, OPA/Rego policies, and Terratest.
Run static checks pre-merge.

9ï¸âƒ£ How do you handle drift between actual and declared infrastructure?

Automated terraform plan checks, drift detection jobs, and GitOps sync alerts.

ğŸ”Ÿ Explain the principle of least privilege in cloud environments.

Give only necessary IAM actions.
Enforce via access analyzer, policy boundaries, and automated audits.

ğŸ” 4. Security & Compliance (8 Questions)

1ï¸âƒ£ How do you ensure container image provenance?

Sign with cosign/Sigstore, verify via admission policy.
Reject unsigned images.

2ï¸âƒ£ Whatâ€™s your strategy for managing secrets across environments?

External secrets + Vault or AWS Secrets Manager, synced by External Secrets Operator.

3ï¸âƒ£ How would you handle vulnerability scanning in runtime?

Use ACS, Falco, or Aqua to continuously scan running containers for CVEs and abnormal syscalls.

4ï¸âƒ£ Whatâ€™s your approach to securing CI/CD pipelines?

Principle of least privilege, isolated runners, secret masking, SBOM generation, and audit trails.

5ï¸âƒ£ How do you detect compromised containers?

Runtime tools like Falco or Sysdig monitor syscalls; look for crypto-mining signatures or privilege escalation.

6ï¸âƒ£ How to ensure cluster compliance (e.g., CIS benchmark)?

Use kube-bench, OPA Gatekeeper policies, or ACS compliance checks.

7ï¸âƒ£ Whatâ€™s Zero Trust in DevOps context?

Every request (human or service) must authenticate & authorize â€” no implicit trust, even within the cluster.

8ï¸âƒ£ How do you prevent supply chain attacks?

Signed commits, verified builds, SBOMs, dependency scanning, and image signature enforcement.

ğŸ“Š 5. Observability & SRE Practices (6 Questions)

1ï¸âƒ£ How do you measure service reliability objectively?

SLOs, SLIs, SLAs â€” latency, availability, error rate.
Implement via Prometheus metrics and alerting.

2ï¸âƒ£ Whatâ€™s your approach to debugging intermittent latency spikes?

Use distributed tracing (Jaeger/Tempo), correlate traces with metrics, analyze slow endpoints.

3ï¸âƒ£ How would you build an observability stack from scratch?

Prometheus + Loki + Tempo + Grafana; automate with Helm + GitOps; standardize dashboards.

4ï¸âƒ£ Whatâ€™s the difference between logging, metrics, and tracing?

Logs = events; Metrics = trends; Traces = request journeys.
Together form full observability triad.

5ï¸âƒ£ How do you detect and prevent alert fatigue?

Use hierarchical alerting: only alert on symptoms, not causes.
Add suppression and proper SLO-based alerts.

6ï¸âƒ£ How do you perform postmortems after incidents?

Blameless retrospectives â†’ timeline, root cause, corrective actions, and automation to prevent recurrence.

ğŸ§­ 6. Leadership & System Design (6 Questions)

1ï¸âƒ£ Your teamâ€™s pipelines work fine but differ widely â€” how do you standardize without bureaucracy?

Create a platform mindset â€” shared templates and best practices via GitOps, not enforcement.

2ï¸âƒ£ How do you handle on-call rotations and burnout?

Use SLO-based alerts, fair schedules, and cross-skill training. Automate repetitive recovery tasks.

3ï¸âƒ£ You find a team deploying manually. How do you guide them to GitOps?

Show ROI: reproducibility, auditability, and rollback safety.
Start with one repo â†’ ArgoCD pilot â†’ scale.

4ï¸âƒ£ Whatâ€™s your strategy for enabling DevSecOps culture in a large org?

Shift-left mindset: embed security in pipeline templates, run regular threat modeling sessions, and automate compliance scans.

5ï¸âƒ£ How do you mentor mid-level engineers in DevOps?

Pair programming, review real incidents, share â€œwhyâ€ behind automation.
Focus on systems thinking, not just YAML.

6ï¸âƒ£ Whatâ€™s the most impactful automation youâ€™ve built that saved real money or time?

(Expect the candidate to tell a measurable story â€” look for ROI thinking.)



Scenario:
Youâ€™ve containerized your app, and it works flawlessly on localhost.
You push it to Kubernetes. Pods are up, image pulled, everything looks clean.

Yet, it never reaches Ready.
1. kubectl logs â†’ nothing.
2. kubectl describe pod â†’ endless restarts, CrashLoopBackOff.
No errors. No logs. No hope.
Now youâ€™ve got 20 minutes before the manager asks: â€œCan we just restart the cluster?â€ ğŸ˜…

So hereâ€™s your test: What would you check next?
ğŸ’¡ Hint: Itâ€™s not the image. Itâ€™s the environment.
Would you look at:
1.    Liveness/Readiness probes?
2.   ConfigMaps or environment variables?
3.   Mounted secrets with wrong paths?
4.   The ENTRYPOINT mismatch in your Dockerfile?
5.   Crash loops triggered by missing dependencies?
The right answer reveals if you think like a tool user or a production engineer.


â€œItâ€™s not the image, itâ€™s the environment.â€

So yes â€” #2 (ConfigMaps/env vars) or #3 (Mounted secrets/config paths) are the most common real-world culprits, followed closely by #4 (bad probes) and #1 (ENTRYPOINT mismatch).


ğŸ§© Scenario 1: App works locally, but in Kubernetes it gives 404

Q: Pods are Running, app container is up, but hitting the Service gives 404.
What do you check?

A:
	â€¢	First verify container port vs Service targetPort â€” mismatch is common.
	â€¢	Check the path rewrite in ingress/route configuration (/app vs /).
	â€¢	Confirm the app actually binds to 0.0.0.0 â€” not localhost.
	â€¢	Finally, check the readiness probe â€” if itâ€™s failing, traffic never routes.

ğŸ§  Rule: If you get 404, itâ€™s usually a networking or routing mismatch â€” not the app itself.

â¸»

ğŸ”¥ Scenario 2: Pod stuck in Pending for 10+ minutes

Q: Whatâ€™s your debugging path?

A:
	â€¢	Run kubectl describe pod <pod> â†’ check for:
	â€¢	0/10 nodes available errors (taints, resource limits, node selectors)
	â€¢	Insufficient CPU/memory
	â€¢	Verify tolerations, affinity, and nodeSelector values.
	â€¢	Check namespace quotas:
kubectl describe quota
	â€¢	If using PVC, see if itâ€™s waiting for a volume:
kubectl get pvc

ğŸ§  Rule: Pending pods are scheduling issues, not image or app issues.

â¸»

ğŸ§  Scenario 3: Pod restarts constantly with OOMKilled

Q: Why does it happen, and how to fix?

A:
	â€¢	OOMKilled â†’ container hit memory limit.

Increase resources.limits.memory or fix memory leaks.
	â€¢	Optional: add --XX:+UseContainerSupport (for JVM apps).

ğŸ§  Rule: Limits are hard stops â€” if the app grows beyond it, kubelet kills it.

â¸»

ğŸ’¾ Scenario 4: ConfigMap updated, but app still uses old config

Q: Why isnâ€™t the new config applied?

A:
	â€¢	Mount-based ConfigMaps update file, but the container doesnâ€™t auto-reload unless app supports it.
	â€¢	Use kubectl rollout restart deployment <name> to reload pods.
	â€¢	For dynamic reloads, add a sidecar (e.g., reloader or configmap-reload).

ğŸ§  Rule: ConfigMap update â‰  pod restart.

â¸»

ğŸ”’ Scenario 5: Secret mount fails, pod stuck in CreateContainerConfigError

Q: What could cause this?

A:
	â€¢	Secret name typo.
	â€¢	Secret missing in the namespace.
	â€¢	Mount path already used by another volume.
	â€¢	Wrong key referenced under secretKeyRef.


ğŸ§­ Scenario 6: Service accessible from inside cluster but not outside

Q: What are the likely causes?

A:
	â€¢	Service type is ClusterIP (internal only).
	â€¢	If using NodePort or LoadBalancer, check firewall or security groups.
	â€¢	For OpenShift â€” verify Route or ingress annotation.
	â€¢	Also check externalTrafficPolicy if using LoadBalancer (can block SNAT).

ğŸ§  Rule: â€œInside works, outside doesnâ€™tâ€ â†’ network exposure misconfiguration.

â¸»

ğŸ§® Scenario 7: Pipeline build fails, but same Dockerfile builds locally

Q: Why?

A:
	â€¢	Pipeline may run as non-root â€” missing permission to write to /tmp or /app.
	â€¢	Missing build arg or env variable (ARG BASE_IMAGE not passed).
	â€¢	Using different Podman/Docker version with stricter syntax.
	â€¢	Check logs for â€œpermission deniedâ€ or â€œCOPY failed: file not foundâ€.

ğŸ§  Rule: CI builds run in different environments, not replicas of localhost.

â¸»

âš™ï¸ Scenario 8: Pods suddenly stuck in CrashLoopBackOff after node reboot

Q: Whatâ€™s your move?

A:
	â€¢	Check node status:
kubectl get nodes
	â€¢	Run kubectl describe node <name> â€” look for disk pressure or not-ready conditions.
	â€¢	Examine persistent volumes â€” some PVs may not have reattached.
	â€¢	Check init containers â€” often fail silently if dependent volume isnâ€™t mounted yet.

ğŸ§  Rule: CrashLoop after reboot often = storage or node taint issue.

â¸»

ğŸ¢ Scenario 9: App feels slow, no CPU/memory issue

Q: Everything is green, but users report slowness. What do you check?

A:
	â€¢	Network latency or DNS issues: kubectl exec -it pod -- ping or dig.
	â€¢	Pod-to-pod network path (CNI issues or MTU mismatch).
	â€¢	Misconfigured readiness probe causing constant rescheduling.
	â€¢	Downstream API slowness â€” check metrics for dependency latency.

ğŸ§  Rule: â€œFeels slowâ€ = latency, not resource starvation.

â¸»

âš¡ Scenario 10: â€œIt works if I exec into the pod and run it manually.â€

Q: Whatâ€™s happening?

A:
	â€¢	ENTRYPOINT or CMD in Dockerfile might be wrong.
	â€¢	App requires shell expansion (bash -c missing).
	â€¢	Relative path (./start.sh) works in shell but not as ENTRYPOINT.
	â€¢	Permissions or missing #!/bin/bash header in script.

ğŸ§  Rule: Manual shell runs inherit environment; ENTRYPOINT runs bare.

âš™ï¸ 1. Pod is in â€œInit:CrashLoopBackOffâ€

Q: Why would an init container crash, even when the main app image is fine?

A:
	â€¢	The init container may reference a Secret or ConfigMap that doesnâ€™t exist yet.
	â€¢	It may have wrong mount path or missing permission on a volume.
	â€¢	The init command might depend on a tool (curl, bash, jq) missing in the image.
	â€¢	Or itâ€™s trying to reach a service (like DB) before itâ€™s up.

ğŸ§  Tip: Run kubectl logs <pod> -c <init-container> â€” not the main container.

â¸»

ğŸ§  2. Pod goes Running â†’ Terminating immediately

Q: What can cause pods to terminate as soon as they start?

A:
	â€¢	The process inside the container exits (e.g. shell scripts finish instantly).
	â€¢	ENTRYPOINT is a short-running command (echo, exit 0).
	â€¢	A preStop hook fails and delays termination â†’ kubelet kills it.
	â€¢	Or PodDisruptionBudget is forcing evictions.

ğŸ§  Rule: Containers must run a long-lived process â€” not a one-shot command.

â¸»

ğŸ’¾ 3. VolumeMount doesnâ€™t show files from ConfigMap

Q: You mounted a ConfigMap but the directory is empty. Why?

A:
	â€¢	ConfigMap is in another namespace.
	â€¢	You forgot to add items: under volumeMounts.
	â€¢	You mounted the volume path overwriting an existing directory (common with /etc/config).
	â€¢	Or pod is using a projected volume that doesnâ€™t refresh automatically.

ğŸ§  Debug:
kubectl exec -it <pod> -- ls /path â†’ confirm mount path inside container.

â¸»

ğŸ§© 4. Job never finishes even though logs say â€œdoneâ€

Q: Why would a Kubernetes Job hang forever?

A:
	â€¢	The main process exited, but a background process (e.g. child PID) is still alive.
	â€¢	The job spec lacks restartPolicy: Never, causing infinite restarts.
	â€¢	Or the app doesnâ€™t send a clean exit code (exit 0).

ğŸ§  Rule: Jobs complete when all containers exit successfully with code 0.

â¸»

ğŸ”„ 5. Deployment updated, but pods still use old image

Q: Why?

A:
	â€¢	The tag (latest) didnâ€™t actually change â†’ Kubernetes doesnâ€™t re-pull by default.
	â€¢	imagePullPolicy is IfNotPresent.
	â€¢	Your registry cached the image digest.
	â€¢	Or Deployment rollout paused.

ğŸ§  Fix:
Use kubectl rollout restart deploy <name> or set imagePullPolicy: Always.

â¸»

ğŸ” 6. â€œImagePullBackOffâ€ but image exists in registry

Q: What are possible causes?

A:
	â€¢	Missing or wrong imagePullSecret.
	â€¢	Secret exists in another namespace.
	â€¢	Registry DNS canâ€™t be resolved.
	â€¢	Image tag has uppercase letters (DockerHub restriction).
	â€¢	The image is private and not publically accessible.

ğŸ§  Debug:
kubectl describe pod â†’ check â€œFailed to pull imageâ€ reason.

â¸»

ğŸ§® 7. HorizontalPodAutoscaler (HPA) not scaling

Q: Pods are under heavy load, but HPA doesnâ€™t react. Why?

A:
	â€¢	Metrics server not installed or not scraping.
	â€¢	Target CPU/Memory metric name mismatch.
	â€¢	HPA bound to Deployment label that changed.
	â€¢	Resource requests/limits missing (HPA needs them for % calculations).

ğŸ§  Check:
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes | jq . â€” ensure metrics exist.

â¸»

ğŸ§  8. Rollout stuck at â€œprogress deadline exceededâ€

Q: Why does your rollout never finish?

A:
	â€¢	Readiness probe keeps failing.
	â€¢	App takes too long to boot and hits progressDeadlineSeconds.
	â€¢	Image tag wrong â†’ crash loops â†’ rollout halted.
	â€¢	Or a maxUnavailable=0 setting with zero room to spin up new pods.

ğŸ§  Fix:
Check probe endpoints, increase progressDeadlineSeconds, and allow surge pods.

â¸»

ğŸŒ 9. Ingress gives 502 Bad Gateway

Q: Common root causes?

A:
	â€¢	Backend Service port mismatch.
	â€¢	Pod not Ready (readiness probe failing).
	â€¢	Path rewrite or host rule incorrect.
	â€¢	Ingress controller misconfigured or crashed.
	â€¢	TLS secret name typo.

ğŸ§  Check:
kubectl describe ingress <name> â†’ look for backend Service references.

â¸»

ğŸ§© 10. CPU throttling but usage is below limit

Q: Why does throttling still happen?

A:
	â€¢	CPU limits are applied via cgroups â€” throttling can occur even if usage < limit due to kernel scheduling.
	â€¢	Node overcommit or noisy neighbors cause throttling.
	â€¢	Fix: remove hard CPU limits or use only requests.

ğŸ§  Rule: CPU limits â‰  performance safety â€” they can cause latency spikes.

â¸»

âš¡ 11. Pod works fine manually but fails in Pipeline-run

Q: What could be happening?

A:
	â€¢	Different runtime (Podman vs Docker).
	â€¢	Missing service account permissions (Tekton, Argo).
	â€¢	Pod security context restrictions (no root).
	â€¢	Missing environment secrets in CI namespace.

ğŸ§  Debug:
Compare environment: kubectl get sa,rolebinding,podsecuritypolicy between namespaces.

â¸»

ğŸ’¡ 12. Liveness probe keeps restarting a slow-start app

Q: How to handle it?

A:
	â€¢	App startup time > probe timeout.
	â€¢	Add initialDelaySeconds or increase timeoutSeconds.
	â€¢	Use startupProbe to handle slow initialization.

ğŸ§  Tip: Use startupProbe â†’ readinessProbe â†’ livenessProbe in sequence.

â¸»

ğŸ’€ 13. Node suddenly marked NotReady

Q: Whatâ€™s your next step?

A:
	â€¢	kubectl describe node <node> â†’ look for:
	â€¢	DiskPressure
	â€¢	MemoryPressure
	â€¢	NetworkUnavailable
	â€¢	SSH in: check systemctl status kubelet, df -h, and logs under /var/log/messages.
	â€¢	Sometimes cloud provider drained node (spot instance).

ğŸ§  Rule: Node issues â‰  Pod issues. Always isolate node-level pressure first.

â¸»

ğŸ§­ 14. Service returns empty response randomly

Q: Why does this happen intermittently?

A:
	â€¢	Readiness probe flapping â†’ traffic sent to half-initialized pods.
	â€¢	Race condition during rollout â€” traffic routed before pod ready.
	â€¢	Client timeout shorter than backend response.
	â€¢	Load balancer sticky sessions misconfigured.

ğŸ§  Fix: Ensure readiness stability; increase probe threshold.

â¸»

ğŸ§¨ 15. CronJob triggers but no logs

Q: Why?

A:
	â€¢	Job completed too quickly and was garbage-collected.
	â€¢	Check with:
kubectl get job --watch
kubectl get cronjob -o yaml
successfulJobsHistoryLimit: 0 â†’ deletes immediately.
	â€¢	Or timezone mismatch â€” schedule runs at odd times.

ğŸ§  Tip: Keep at least 1 successful job for log visibility.

Q pod stuck in terminating state but not getting deleted

A pod stuck in Terminating usually means Kubernetes is waiting for a finalizer or graceful cleanup to finish.
Common causes include PVC detach issues, sidecar (Istio/Tekton) finalizers, or a NotReady node.
Check with kubectl get pod <pod> -o json | jq '.metadata.finalizers'.
If safe, remove it using kubectl patch pod <pod> -p '{"metadata":{"finalizers":null}}' --type=merge.


Q you commit secret in git now what?
If I commit a secret by mistake, Iâ€™ll immediately revoke or rotate it, remove it from Git history using git filter-repo, force push the clean history, and inform collaborators to re-clone. Then Iâ€™ll add pre-commit secret scanning to prevent future leaks.